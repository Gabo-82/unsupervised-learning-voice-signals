{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ef9d075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBCV Score (muestra 10%): 0.84  (mayor mejor)\n",
      "Silhouette Score (muestra 10%): 0.7149\n",
      "Calinski-Harabasz Score: 18193.12  (mayor mejor)\n",
      "Davies-Bouldin Score: 0.29  (menor mejor)\n"
     ]
    }
   ],
   "source": [
    "#Evaluar hdbscan + umap\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "from hdbscan.validity import validity_index\n",
    "\n",
    "# 1. Cargar el parquet\n",
    "df = pd.read_parquet('parquets/no_wind/hdbscan_results/hdbscan3000_100_umap100_5_imputed.parquet')\n",
    "features = ['zcrall', 'normpeakall', 'spectralTiltall', 'LHratioall', \n",
    "            'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "# 2. Reconstruir X_umap\n",
    "umap_cols = [col for col in df.columns if col.startswith('umap_')]\n",
    "X_umap = df[umap_cols].values\n",
    "\n",
    "# 3. Tomar los labels de HDBSCAN\n",
    "cluster_labels = df['hdbscan_label'].values\n",
    "\n",
    "# 4. Seguir con el mismo flujo de máscara, resample y evaluación\n",
    "mask = cluster_labels != -1\n",
    "X_clustered = X_umap[mask]\n",
    "labels_clustered = cluster_labels[mask]\n",
    "\n",
    "X_sampled, _, y_sampled, _ = train_test_split(\n",
    "    X_clustered, labels_clustered,\n",
    "    train_size=10000,\n",
    "    stratify=labels_clustered,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_sampled_64 = X_sampled.astype(np.float64)\n",
    "dbcv_score_sampled = validity_index(X_sampled_64, y_sampled, metric='euclidean')\n",
    "print(f\"DBCV Score (muestra 10%): {dbcv_score_sampled:.2f}  (mayor mejor)\")\n",
    "sil_score = silhouette_score(X_sampled, y_sampled)\n",
    "print(f\"Silhouette Score (muestra 10%): {sil_score:.4f}\")\n",
    "ch_score = calinski_harabasz_score(X_sampled, y_sampled)\n",
    "print(f\"Calinski-Harabasz Score: {ch_score:.2f}  (mayor mejor)\")\n",
    "db_score = davies_bouldin_score(X_sampled, y_sampled)\n",
    "print(f\"Davies-Bouldin Score: {db_score:.2f}  (menor mejor)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ef863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "c:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.19 TiB for an array with shape (405132, 405132) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m persistence = clusterer.cluster_persistence_\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# 8. Calcular validity index (DBCV)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m dbcv_score = \u001b[43mvalidity_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclusterer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlabels_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meuclidean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 9. Mostrar resultados\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCluster persistence (por cluster):\u001b[39m\u001b[33m\"\u001b[39m, persistence)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hdbscan\\validity.py:361\u001b[39m, in \u001b[36mvalidity_index\u001b[39m\u001b[34m(X, labels, metric, d, per_cluster_scores, mst_raw_dist, verbose, **kwd_args)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.sum(labels == cluster_id) == \u001b[32m0\u001b[39m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    360\u001b[39m distances_for_mst, core_distances[\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     cluster_id] = \u001b[43mdistances_between_points\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcluster_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mno_coredist\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmst_raw_dist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_max_raw_to_coredist_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwd_args\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m mst_nodes[cluster_id], mst_edges[cluster_id] = \\\n\u001b[32m    373\u001b[39m     internal_minimum_spanning_tree(distances_for_mst)\n\u001b[32m    374\u001b[39m density_sparseness[cluster_id] = mst_edges[cluster_id].T[\u001b[32m2\u001b[39m].max()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hdbscan\\validity.py:117\u001b[39m, in \u001b[36mdistances_between_points\u001b[39m\u001b[34m(X, labels, cluster_id, metric, d, no_coredist, print_max_raw_to_coredist_ratio, **kwd_args)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m     subset_X = X[labels == cluster_id, :]\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     distance_matrix = \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m                                         \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwd_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     d = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m no_coredist:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2480\u001b[39m, in \u001b[36mpairwise_distances\u001b[39m\u001b[34m(X, Y, metric, n_jobs, force_all_finite, ensure_all_finite, **kwds)\u001b[39m\n\u001b[32m   2477\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m distance.squareform(distance.pdist(X, metric=metric, **kwds))\n\u001b[32m   2478\u001b[39m     func = partial(distance.cdist, metric=metric, **kwds)\n\u001b[32m-> \u001b[39m\u001b[32m2480\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1973\u001b[39m, in \u001b[36m_parallel_pairwise\u001b[39m\u001b[34m(X, Y, func, n_jobs, **kwds)\u001b[39m\n\u001b[32m   1970\u001b[39m X, Y, dtype = _return_float_dtype(X, Y)\n\u001b[32m   1972\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1973\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[32m   1976\u001b[39m fd = delayed(_dist_wrapper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:189\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:388\u001b[39m, in \u001b[36meuclidean_distances\u001b[39m\u001b[34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[39m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared.shape != (\u001b[32m1\u001b[39m, Y.shape[\u001b[32m0\u001b[39m]):\n\u001b[32m    383\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    384\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    385\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    386\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:424\u001b[39m, in \u001b[36m_euclidean_distances\u001b[39m\u001b[34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[39m\n\u001b[32m    421\u001b[39m     distances = _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     distances = -\u001b[32m2\u001b[39m * \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m     distances += XX\n\u001b[32m    426\u001b[39m     distances += YY\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:203\u001b[39m, in \u001b[36msafe_sparse_dot\u001b[39m\u001b[34m(a, b, dense_output)\u001b[39m\n\u001b[32m    201\u001b[39m         ret = xp.tensordot(a, b, axes=[-\u001b[32m1\u001b[39m, b_axis])\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     ret = \u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    206\u001b[39m     sparse.issparse(a)\n\u001b[32m    207\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m sparse.issparse(b)\n\u001b[32m    208\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[32m    209\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[33m\"\u001b[39m\u001b[33mtoarray\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m ):\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.toarray()\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 1.19 TiB for an array with shape (405132, 405132) and data type float64"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#persistence\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import hdbscan\n",
    "from hdbscan.validity import validity_index\n",
    "import numpy as np\n",
    "\n",
    "# 1. Cargar el parquet\n",
    "df = pd.read_parquet('parquets/no_wind/hdbscan_results/hdbscan1000_50_umap30_9_imputed.parquet')\n",
    "features = ['zcrall', 'normpeakall', 'spectralTiltall', 'LHratioall', \n",
    "            'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "# 2. Reconstruir X_umap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d194f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score (muestra 10%): 0.0607\n",
      "Calinski-Harabasz Score: 2839.86  (mayor mejor)\n",
      "Davies-Bouldin Score: 1.70  (menor mejor)\n",
      "Número de clusters: 7\n",
      "Porcentaje de ruido: 0.18%\n"
     ]
    }
   ],
   "source": [
    "#Evaluar solo HDBSCAN\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. Cargar el parquet con HDBSCAN labels\n",
    "df = pd.read_parquet('parquets/real_timestamps/hdbscan_results/hdbscan300_50_umap30_9_imputed.parquet')\n",
    "\n",
    "# 2. Definir features\n",
    "features = ['zcrall', 'normpeakall', 'spectralTiltall', 'LHratioall', \n",
    "            'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "X = df[features].values\n",
    "\n",
    "# 3. Escalar features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Obtener labels de clustering\n",
    "cluster_labels = df['hdbscan_label'].values\n",
    "\n",
    "# 5. Filtrar puntos con cluster válido (excluye ruido -1)\n",
    "mask = cluster_labels != -1\n",
    "X_clustered = X_scaled[mask]\n",
    "labels_clustered = cluster_labels[mask]\n",
    "\n",
    "# 6. Resamplear para evaluación rápida\n",
    "X_sampled, _, y_sampled, _ = train_test_split(\n",
    "    X_clustered, labels_clustered,\n",
    "    train_size=0.1,\n",
    "    stratify=labels_clustered,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 7. Calcular métricas\n",
    "sil_score = silhouette_score(X_sampled, y_sampled)\n",
    "ch_score = calinski_harabasz_score(X_sampled, y_sampled)\n",
    "db_score = davies_bouldin_score(X_sampled, y_sampled)\n",
    "\n",
    "# 8. Mostrar resultados\n",
    "print(f\"Silhouette Score (muestra 10%): {sil_score:.4f}\")\n",
    "print(f\"Calinski-Harabasz Score: {ch_score:.2f}  (mayor mejor)\")\n",
    "print(f\"Davies-Bouldin Score: {db_score:.2f}  (menor mejor)\")\n",
    "print(f\"Número de clusters: {len(np.unique(labels_clustered))}\")\n",
    "print(f\"Porcentaje de ruido: {100 * np.sum(cluster_labels == -1) / len(cluster_labels):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8a85f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>hdbscan_label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>condition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject_week</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NF031_Control</th>\n",
       "      <td>0.237799</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.756829</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NF038_Control</th>\n",
       "      <td>0.149371</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.003302</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.845018</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NF111_Control</th>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.120472</td>\n",
       "      <td>0.871102</td>\n",
       "      <td>0.006230</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NF126_Control</th>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.150981</td>\n",
       "      <td>0.846608</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NF134_Control</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.254863</td>\n",
       "      <td>0.743926</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>Control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "hdbscan_label         0         1         2         3         4 condition\n",
       "subject_week                                                             \n",
       "NF031_Control  0.237799  0.000941  0.003147  0.001284  0.756829   Control\n",
       "NF038_Control  0.149371  0.000336  0.003302  0.001972  0.845018   Control\n",
       "NF111_Control  0.002163  0.120472  0.871102  0.006230  0.000033   Control\n",
       "NF126_Control  0.000040  0.150981  0.846608  0.002138  0.000234   Control\n",
       "NF134_Control  0.000061  0.254863  0.743926  0.001090  0.000061   Control"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agrupar directamente por sujeto + semana real\n",
    "import pandas as pd\n",
    "df = pd.read_parquet('parquets/no_wind/hdbscan_results/hdbscan1000_50_umap30_5_0p1imputed.parquet')\n",
    "df_valid = df[df['hdbscan_label'] != -1]\n",
    "df_valid['subject_week'] = df_valid['subject_id'] + \"_\" + df_valid['week']\n",
    "\n",
    "# Obtener tabla de frecuencias y normalizar\n",
    "cluster_dist = pd.crosstab(df_valid['subject_week'], df_valid['hdbscan_label'])\n",
    "cluster_freq = cluster_dist.div(cluster_dist.sum(axis=1), axis=0)\n",
    "\n",
    "# Reasignar condición usando el DataFrame original\n",
    "subject_week_to_condition = df_valid.drop_duplicates('subject_week').set_index('subject_week')['week']\n",
    "cluster_freq['condition'] = cluster_freq.index.map(subject_week_to_condition)\n",
    "\n",
    "cluster_freq.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a679cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdbscan_label         0         1         2         3         4\n",
      "condition                                                      \n",
      "Control        0.077887  0.105519  0.493617  0.002543  0.320435\n",
      "Post           0.000081  0.175433  0.819173  0.004847  0.000466\n",
      "Pre            0.000112  0.124530  0.866647  0.008227  0.000484\n",
      "JS(Pre || Control): 0.4039\n",
      "JS(Post || Control): 0.4014\n",
      "JS(Post || Pre): 0.0522\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Promedio de vectores por condición\n",
    "means = cluster_freq.groupby('condition').mean()\n",
    "print(means)\n",
    "\n",
    "# JS distance entre cada par\n",
    "js_pre_control = jensenshannon(means.loc['Pre'], means.loc['Control'])\n",
    "js_post_control = jensenshannon(means.loc['Post'], means.loc['Control'])\n",
    "js_post_pre = jensenshannon(means.loc['Post'], means.loc['Pre'])\n",
    "\n",
    "print(f\"JS(Pre || Control): {js_pre_control:.4f}\")\n",
    "print(f\"JS(Post || Control): {js_post_control:.4f}\")\n",
    "print(f\"JS(Post || Pre): {js_post_pre:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf7f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
