{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "904de47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_parquet(\"parquets/no_wind/features_selected_imputed.parquet\")\n",
    "features = ['zcrall', 'normpeakall', 'spectralTiltall', 'LHratioall', 'periodicity', \n",
    "            'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "umap_model = umap.UMAP(n_neighbors=50, min_dist=0.1,\n",
    "               metric='euclidean', low_memory=True)\n",
    "X = df[features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_umap = umap_model.fit_transform(X_scaled)\n",
    "df['umap_X'] = X_umap[:, 0]\n",
    "df['umap_Y'] = X_umap[:, 1]\n",
    "df.to_parquet(\"parquets/no_wind/umap_results/features_selected_imputed_umap100.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2ce7abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "c:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_parquet(\"parquets/features_selected_imputed.parquet\")\n",
    "features = ['zcrall', 'normpeakall', 'spectralTiltall', 'LHratioall', 'periodicity', \n",
    "            'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "\n",
    "n_per_grp = 30_000\n",
    "dfs = []\n",
    "for grp, sub in df.groupby('week'):          # 'Control', 'Pre', 'Post'\n",
    "    dfs.append(sub.sample(n=n_per_grp, random_state=42, replace=False))\n",
    "df_bal = pd.concat(dfs).reset_index(drop=True)\n",
    "df_bal = shuffle(df_bal, random_state=42)      # mezclar\n",
    "\n",
    "# 2) Escalado\n",
    "scaler = StandardScaler()\n",
    "X_bal = scaler.fit_transform(df_bal[features])\n",
    "\n",
    "# 3) Entrenar UMAP en el set balanceado\n",
    "um = umap.UMAP(n_neighbors=30, min_dist=0.1,\n",
    "               metric='euclidean', random_state=42)\n",
    "emb_bal = um.fit_transform(X_bal)\n",
    "df_bal['umap_X'], df_bal['umap_Y'] = emb_bal[:,0], emb_bal[:,1]\n",
    "df_bal.to_parquet(\"parquets/umap_results/features_selected_imputed_umap30_bal.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914290e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- carga de datos (idéntica a tu código) -------------------------------\n",
    "df_bal = pd.read_parquet(\"parquets/no_wind/umap_results/features_selected_imputed_umap50.parquet\")\n",
    "df_hdbscan = pd.read_parquet('parquets/no_wind/hdbscan_results/hdbscan1000_50_umap30_5_0p1imputed.parquet')\n",
    "df_bal['hdbscan_label'] = df_hdbscan['hdbscan_label']\n",
    "\n",
    "df_control = df_bal[df_bal['week'] == 'Control']\n",
    "df_pre     = df_bal[df_bal['week'] == 'Pre']\n",
    "df_post    = df_bal[df_bal['week'] == 'Post']\n",
    "\n",
    "# --- ploteo --------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
    "\n",
    "# Primer scatterplot (con leyenda)\n",
    "sns.scatterplot(\n",
    "    data=df_control, x='umap_X', y='umap_Y',\n",
    "    ax=axes[0], hue='hdbscan_label',\n",
    "    palette='Set2', alpha=0.5\n",
    ")\n",
    "axes[0].set_title('Control')\n",
    "\n",
    "# Siguientes scatterplots SIN leyenda\n",
    "for ax, data, title in zip(\n",
    "        axes[1:],\n",
    "        [df_pre, df_post],\n",
    "        ['Pre', 'Post']):\n",
    "    sns.scatterplot(\n",
    "        data=data, x='umap_X', y='umap_Y',\n",
    "        ax=ax, hue='hdbscan_label',\n",
    "        palette='Set2', alpha=0.5, legend=False\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "\n",
    "# --- leyenda única y reducida -------------------------------------------\n",
    "# 1. Extraer handles/labels de la leyenda del primer eje\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "\n",
    "# 2. Crear leyenda de figura (tamaño pequeño)\n",
    "fig.legend(\n",
    "    handles, labels,\n",
    "    loc='lower center',          # elige la posición que mejor se adapte\n",
    "    bbox_to_anchor=(0.5, 1.02),  # fuera del área de dibujo\n",
    "    ncol=min(5, len(labels)),    # distribuye en varias columnas si hay muchas clases\n",
    "    frameon=False,\n",
    "    fontsize='x-small',          # más pequeño que default\n",
    "    title='hdbscan_label'        # opcional: título de la leyenda\n",
    ")\n",
    "\n",
    "# 3. Eliminar la leyenda sobrante del primer eje\n",
    "axes[0].get_legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523b3a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m df_vis = pd.read_parquet(\u001b[33m\"\u001b[39m\u001b[33mparquets/no_wind/umap_results/features_selected_imputed_umap50.parquet\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparquets/no_wind/hdbscan_results/hdbscan1000_50_umap30_5_0p1imputed.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df_vis[\u001b[33m'\u001b[39m\u001b[33mhdbscan_label\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mhdbscan_label\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m df_vis[\u001b[33m'\u001b[39m\u001b[33mCAPE_overall\u001b[39m\u001b[33m'\u001b[39m] = df[\u001b[33m'\u001b[39m\u001b[33mCAPE_overall\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    664\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    665\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    267\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    268\u001b[39m     path,\n\u001b[32m    269\u001b[39m     filesystem,\n\u001b[32m    270\u001b[39m     storage_options=storage_options,\n\u001b[32m    271\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    272\u001b[39m )\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m     result = pa_table.to_pandas(**to_pandas_kwargs)\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1831\u001b[39m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[32m   1832\u001b[39m     dataset = ParquetFile(\n\u001b[32m   1833\u001b[39m         source, read_dictionary=read_dictionary,\n\u001b[32m   1834\u001b[39m         memory_map=memory_map, buffer_size=buffer_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1840\u001b[39m         page_checksum_verification=page_checksum_verification,\n\u001b[32m   1841\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1843\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1844\u001b[39m \u001b[43m                    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[39m, in \u001b[36mParquetDataset.read\u001b[39m\u001b[34m(self, columns, use_threads, use_pandas_metadata)\u001b[39m\n\u001b[32m   1477\u001b[39m         index_columns = [\n\u001b[32m   1478\u001b[39m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[32m   1479\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1480\u001b[39m         ]\n\u001b[32m   1481\u001b[39m         columns = (\n\u001b[32m   1482\u001b[39m             \u001b[38;5;28mlist\u001b[39m(columns) + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) - \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[32m   1483\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1485\u001b[39m table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_expression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1487\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_threads\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[32m   1491\u001b[39m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\_dataset.pyx:574\u001b[39m, in \u001b[36mpyarrow._dataset.Dataset.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3865\u001b[39m, in \u001b[36mpyarrow._dataset.Scanner.to_table\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gdiaz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyarrow\\error.pxi:89\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_vis = pd.read_parquet(\"parquets/umap_results/features_selected_imputed_umap30.parquet\")\n",
    "df = pd.read_parquet('parquets/hdbscan_results/hdbscan1000_200_umap30_9_imputed.parquet')\n",
    "\n",
    "df_vis['hdbscan_label'] = df['hdbscan_label']\n",
    "df_vis['CAPE_overall'] = df['CAPE_overall']\n",
    "df_vis['vrqol_total'] = df['vrqol_total']\n",
    "df_pre = df_vis[df_vis['week'] == 'Pre']\n",
    "df_post = df_vis[df_vis['week'] == 'Post']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "sns.scatterplot(\n",
    "    data=df_pre, x='umap_X', y='umap_Y',\n",
    "    ax=axes[0], hue='CAPE_overall',\n",
    "    palette='Set2', alpha=0.6, s=3\n",
    ")\n",
    "axes[0].set_title('Pre')\n",
    "sns.scatterplot(\n",
    "    data=df_post, x='umap_X', y='umap_Y',\n",
    "    ax=axes[1], hue='CAPE_overall',\n",
    "    palette='Set2', alpha=0.6, s=3\n",
    ")\n",
    "axes[1].set_title('Post')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3652d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# 1. Cargar datos\n",
    "df = pd.read_parquet(\"parquets/no_wind/umap_results/features_selected_imputed_umap50.parquet\")\n",
    "hdbscan = pd.read_parquet('parquets/no_wind/hdbscan_results/hdbscan1000_50_umap30_5_0p1imputed.parquet')\n",
    "df = df.reset_index(drop=True)\n",
    "hdbscan = hdbscan.reset_index(drop=True)\n",
    "\n",
    "# Agregar la columna 'hdbscan_label' al df con UMAP\n",
    "df['hdbscan_label'] = hdbscan['hdbscan_label']\n",
    "\n",
    "# 2. Agregar columna de día (sin hora)\n",
    "df['day'] = pd.to_datetime(df['ts']).dt.date\n",
    "\n",
    "# 3. Función para crear figura con slider para un sujeto\n",
    "def create_slider_plot(subject_id):\n",
    "    df_sub = df[df['subject_id'] == subject_id].copy()\n",
    "    unique_days = sorted(df_sub['day'].unique())\n",
    "\n",
    "    fig_dict = {\n",
    "        \"data\": [],\n",
    "        \"layout\": {},\n",
    "        \"frames\": []\n",
    "    }\n",
    "\n",
    "    # Layout\n",
    "    fig_dict[\"layout\"][\"xaxis\"] = {\"title\": \"UMAP 1\"}\n",
    "    fig_dict[\"layout\"][\"yaxis\"] = {\"title\": \"UMAP 2\"}\n",
    "    fig_dict[\"layout\"][\"hovermode\"] = \"closest\"\n",
    "    fig_dict[\"layout\"][\"title\"] = f\"UMAP por día - {subject_id}\"\n",
    "    fig_dict[\"layout\"][\"height\"] = 800\n",
    "    fig_dict[\"layout\"][\"updatemenus\"] = [{\n",
    "        \"type\": \"buttons\",\n",
    "        \"buttons\": [{\n",
    "            \"label\": \"Play\",\n",
    "            \"method\": \"animate\",\n",
    "            \"args\": [None, {\"frame\": {\"duration\": 1000, \"redraw\": True},\n",
    "                            \"fromcurrent\": True}]\n",
    "        }]\n",
    "    }]\n",
    "\n",
    "    # Slider setup\n",
    "    sliders_dict = {\n",
    "        \"active\": 0,\n",
    "        \"steps\": [],\n",
    "        \"x\": 0.1,\n",
    "        \"len\": 0.9,\n",
    "        \"xanchor\": \"left\",\n",
    "        \"y\": 0,\n",
    "        \"yanchor\": \"top\",\n",
    "        \"pad\": {\"b\": 10},\n",
    "        \"currentvalue\": {\"prefix\": \"Día: \"}\n",
    "    }\n",
    "\n",
    "    # Inicializar con el primer día\n",
    "    first_day = unique_days[0]\n",
    "    df_day = df_sub[df_sub['day'] == first_day]\n",
    "\n",
    "    scatter = go.Scatter(\n",
    "    x=df_day[\"umap_X\"],\n",
    "    y=df_day[\"umap_Y\"],\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        color=df_day[\"hdbscan_label\"],  # ahora sí usas la etiqueta\n",
    "        colorscale=\"Viridis\",\n",
    "        size=5,\n",
    "        colorbar=dict(title=\"Cluster\")\n",
    "    ),\n",
    "    text=df_day[\"ts\"],  # para que se vea el cluster al pasar el mouse\n",
    "    hoverinfo=\"text\"\n",
    ")\n",
    "    fig_dict[\"data\"] = [scatter]\n",
    "\n",
    "    # Crear frames por día\n",
    "    for day in unique_days:\n",
    "        df_day = df_sub[df_sub['day'] == day]\n",
    "        frame = go.Frame(\n",
    "            data=[go.Scatter(\n",
    "                x=df_day[\"umap_X\"],\n",
    "                y=df_day[\"umap_Y\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(\n",
    "                color=df_day[\"hdbscan_label\"],  # ahora sí usas la etiqueta\n",
    "                colorscale=\"Viridis\",\n",
    "                size=5,\n",
    "                colorbar=dict(title=\"Cluster\")\n",
    "    ),\n",
    "    text=df_day[\"ts\"],  # para que se vea el cluster al pasar el mouse\n",
    "    hoverinfo=\"text\"\n",
    "            )],\n",
    "            name=str(day)\n",
    "        )\n",
    "        fig_dict[\"frames\"].append(frame)\n",
    "\n",
    "        slider_step = {\n",
    "            \"args\": [\n",
    "                [str(day)],\n",
    "                {\"frame\": {\"duration\": 0, \"redraw\": True},\n",
    "                 \"mode\": \"immediate\"}\n",
    "            ],\n",
    "            \"label\": str(day),\n",
    "            \"method\": \"animate\"\n",
    "        }\n",
    "        sliders_dict[\"steps\"].append(slider_step)\n",
    "\n",
    "    fig_dict[\"layout\"][\"sliders\"] = [sliders_dict]\n",
    "\n",
    "    return go.Figure(fig_dict)\n",
    "\n",
    "# 4. Crear figura para cada sujeto\n",
    "fig_nf = create_slider_plot(\"NF134\")\n",
    "fig_pf = create_slider_plot(\"PF134\")\n",
    "\n",
    "# 5. Mostrar\n",
    "fig_nf.show()\n",
    "fig_pf.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05d42438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61689, 19)\n"
     ]
    }
   ],
   "source": [
    "nf031 = df[df['subject_id'] == \"NF134\"].copy()\n",
    "print(nf031.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
