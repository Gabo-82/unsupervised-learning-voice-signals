{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958aac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "\n",
    "df = pd.read_parquet(\"parquets/features_selected_imputed.parquet\")\n",
    "features = ['zcrall', 'normpeakall', 'spectralTiltall', 'LHratioall', 'periodicity', 'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "\n",
    "#sub sampleo 10% de los datos\n",
    "df = df.sample(frac=0.1, random_state=42)\n",
    "print(\"Tamaño del dataframe después de sub muestreo:\", df.shape)\n",
    "\n",
    "X = df[features]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Clustering con HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=1000)\n",
    "cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "df['hdbscan_label'] = cluster_labels\n",
    "df.to_parquet(\"parquets/hdbscan_results/hdbscan1000_imputed.parquet\")\n",
    "\n",
    "print(\"Número de clusters encontrados:\", len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0))\n",
    "print(\"Número de puntos de ruido:\", list(cluster_labels).count(-1))\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a20737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Cargar tus datos\n",
    "df = pd.read_parquet(\"parquets/real_timestamps/features_selected.parquet\")\n",
    "\n",
    "features = ['zcrall','normpeakall','spectralTiltall','LHratioall',\n",
    "            'periodicity','cppall','acflow','oq','naq','h1h2']\n",
    "\n",
    "X = df[features]\n",
    "\n",
    "# 2. Imputar faltantes con mediana\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns=features)\n",
    "df[features] = X_imputed_df[features]\n",
    "df.to_parquet(\"parquets/real_timestamps/features_selected_imputed.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffaecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umap con subsample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import umap\n",
    "import hdbscan\n",
    "from hdbscan.validity import validity_index\n",
    "\n",
    "# === 0. Cargar y definir columnas ===\n",
    "df = pd.read_parquet(\"parquets/no_wind/features_selected_imputed.parquet\")\n",
    "features = ['zcrall', 'normpeakall', 'LHratioall', 'spectralTiltall',\n",
    "            'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "\n",
    "# === 1. Submuestreo estratificado para entrenamiento (ej. 60k por grupo) ===\n",
    "n_per_grp = 200_000\n",
    "dfs = []\n",
    "for grp, sub in df.groupby('week'):  # 'Control', 'Pre', 'Post'\n",
    "    dfs.append(sub.sample(n=n_per_grp, random_state=42, replace=False))\n",
    "df_bal = pd.concat(dfs).reset_index(drop=True)\n",
    "df_bal = shuffle(df_bal, random_state=42)\n",
    "\n",
    "# === 2. Escalado global ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled_full = scaler.fit_transform(df[features])         # Todo el dataset\n",
    "X_scaled_sample = scaler.transform(df_bal[features])       # Solo el sample\n",
    "\n",
    "# === 3. UMAP fit en muestra, transform en todo ===\n",
    "umap_model = umap.UMAP(\n",
    "    n_neighbors=100,\n",
    "    min_dist=0.1,\n",
    "    n_components=9,\n",
    "    random_state=42,\n",
    ")\n",
    "X_umap_sample = umap_model.fit_transform(X_scaled_sample)\n",
    "X_umap_full = umap_model.transform(X_scaled_full)\n",
    "print(\"UMAP fit/transform completado. Dimensiones:\", X_umap_full.shape)\n",
    "\n",
    "# === 4. HDBSCAN en el embedding del sample ===\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=1000,\n",
    "    min_samples=50,\n",
    "    prediction_data=True,\n",
    ").fit(X_umap_sample)\n",
    "\n",
    "# === 5. Aproximar etiquetas para todo el set ===\n",
    "labels_full, strengths_full = hdbscan.approximate_predict(clusterer, X_umap_full)\n",
    "\n",
    "# === 6. Añadir columnas al DataFrame original ===\n",
    "for i in range(X_umap_full.shape[1]):\n",
    "    df[f'umap_{i}'] = X_umap_full[:, i]\n",
    "df['hdbscan_label'] = labels_full\n",
    "df['cluster_confidence'] = strengths_full\n",
    "\n",
    "# === 7. Informes ===\n",
    "print(\"Clusters encontrados (en muestra):\", clusterer.labels_.max() + 1)\n",
    "print(\"Ruido en total:\", np.sum(labels_full == -1))\n",
    "\n",
    "# === 8. Guardar ===\n",
    "df.to_parquet('parquets/no_wind/hdbscan_results/hdbscan1000_50_umap100_9_imputed.parquet', index=False)\n",
    "print(\"Guardado exitosamente\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b2a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import umap\n",
    "import hdbscan\n",
    "from hdbscan.validity import validity_index\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# === 0. Cargar datos ===\n",
    "df = pd.read_parquet(\"parquets/no_wind/features_selected_imputed.parquet\")\n",
    "\n",
    "features = ['zcrall', 'normpeakall', 'LHratioall', 'spectralTiltall',\n",
    "            'cppall', 'acflow', 'oq', 'naq', 'h1h2']\n",
    "\n",
    "# === 1. Submuestreo estratificado (se reutiliza) ===\n",
    "n_per_grp = 200_000\n",
    "dfs = [sub.sample(n=n_per_grp, random_state=42, replace=False)\n",
    "       for _, sub in df.groupby('week')]\n",
    "df_bal = shuffle(pd.concat(dfs).reset_index(drop=True), random_state=42)\n",
    "\n",
    "# === 2. Escalado global ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled_full   = scaler.fit_transform(df[features])\n",
    "X_scaled_sample = scaler.transform(df_bal[features])\n",
    "\n",
    "# === 3. Definir grilla de hiperparámetros ===\n",
    "grid = list(product(\n",
    "    [5, 9],          # n_components\n",
    "    [30, 100],       # n_neighbors\n",
    "    [0.05, 0.1],     # min_dist\n",
    "    [1000, 3000],    # min_cluster_size\n",
    "    [50, 100]        # min_samples\n",
    "))\n",
    "\n",
    "# Carpeta de resultados\n",
    "out_path = Path(\"parquets/no_wind/hdbscan_results\")\n",
    "out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_comp, n_neigh, m_dist, min_c, min_s in grid:\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n▶️  UMAP(nc={n_comp}, nn={n_neigh}, md={m_dist}) | \"\n",
    "          f\"HDB(min_c={min_c}, min_s={min_s})\")\n",
    "\n",
    "    # 3.1 Fit-transform UMAP en sample\n",
    "    umap_model = umap.UMAP(\n",
    "        n_neighbors=n_neigh,\n",
    "        min_dist=m_dist,\n",
    "        n_components=n_comp,\n",
    "        random_state=42,\n",
    "        low_memory=True\n",
    "    )\n",
    "    X_umap_sample = umap_model.fit_transform(X_scaled_sample)\n",
    "    X_umap_full   = umap_model.transform(X_scaled_full)\n",
    "\n",
    "    # 3.2 HDBSCAN en sample\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_c,\n",
    "        min_samples=min_s,\n",
    "        prediction_data=True\n",
    "    ).fit(X_umap_sample)\n",
    "\n",
    "    # 3.3 Aproximar etiquetas para todo el dataset\n",
    "    labels_full, strengths_full = hdbscan.approximate_predict(clusterer, X_umap_full)\n",
    "\n",
    "    # 3.4 Métricas (muestra estratificada de 10 000 puntos distintos de ruido)\n",
    "    mask_clustered = labels_full != -1\n",
    "    X_clustered = X_umap_full[mask_clustered]\n",
    "    y_clustered = labels_full[mask_clustered]\n",
    "\n",
    "    X_sampled, _, y_sampled, _ = train_test_split(\n",
    "        X_clustered, y_clustered,\n",
    "        train_size=10_000, stratify=y_clustered, random_state=42\n",
    "    )\n",
    "\n",
    "    dbcv  = validity_index(X_sampled.astype(np.float64), y_sampled, metric='euclidean')\n",
    "    sil   = silhouette_score(X_sampled, y_sampled)\n",
    "    ch    = calinski_harabasz_score(X_sampled, y_sampled)\n",
    "    db    = davies_bouldin_score(X_sampled, y_sampled)\n",
    "\n",
    "    # 3.5 Guardar parquet con etiquetas y embedding\n",
    "    file_name = f\"hdbscan{min_c}_{min_s}_umap{n_neigh}_{n_comp}_{str(m_dist).replace('.','p')}imputed.parquet\"\n",
    "    df_out = df.copy()\n",
    "    for i in range(n_comp):\n",
    "        df_out[f'umap_{i}'] = X_umap_full[:, i]\n",
    "    df_out['hdbscan_label']      = labels_full\n",
    "    df_out['cluster_confidence'] = strengths_full\n",
    "    df_out.to_parquet(out_path / file_name, index=False)\n",
    "\n",
    "    # 3.6 Registrar resultados\n",
    "    results.append({\n",
    "        'n_components': n_comp,\n",
    "        'n_neighbors': n_neigh,\n",
    "        'min_dist': m_dist,\n",
    "        'min_cluster_size': min_c,\n",
    "        'min_samples': min_s,\n",
    "        'clusters_found': clusterer.labels_.max() + 1,\n",
    "        'noise_ratio_sample': np.mean(clusterer.labels_ == -1),\n",
    "        'noise_ratio_full': np.mean(labels_full == -1),\n",
    "        'DBCV': dbcv,\n",
    "        'Silhouette': sil,\n",
    "        'Calinski_Harabasz': ch,\n",
    "        'Davies_Bouldin': db,\n",
    "        'runtime_sec': round(time.time() - t0, 1)\n",
    "    })\n",
    "\n",
    "    print(f\"  ➜ DBCV={dbcv:.3f}  Sil={sil:.3f}  CH={ch:.1f}  DB={db:.2f} \"\n",
    "          f\"| clusters={clusterer.labels_.max()+1}  ruido_full={np.mean(labels_full == -1):.2%}\")\n",
    "\n",
    "# === 4. Guardar CSV de resultados ===\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('gridsearch_umap_hdbscan_scores.csv', index=False)\n",
    "print(\"\\n✅ Grid-search finalizado. Resultados guardados en 'gridsearch_umap_hdbscan_scores.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0cf50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_sampled, _, y_sampled, _ = train_test_split(\n",
    "    X_valid, labels_valid,\n",
    "    train_size=0.1,\n",
    "    stratify=labels_valid,\n",
    "    random_state=42\n",
    ")\n",
    "X_sampled_64 = X_sampled.astype(np.float64)\n",
    "\n",
    "dbcv_score_sampled = validity_index(X_sampled_64, y_sampled, metric='euclidean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f24427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('gridsearch_umap_hdbscan_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
